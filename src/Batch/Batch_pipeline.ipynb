{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Batch Processing Pipeline - Used car market analysis",
   "id": "b15189313aefb7af"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "ðŸ› ï¸ Windows Hadoop/Spark Setup\n",
    "What this does: This script automates the installation of Hadoop binaries on Windows. It creates the directory C:\\hadoop\\bin, downloads the required winutils.exe and hadoop.dll files, and sets the HADOOP_HOME environment variable.\n",
    "\n",
    "Why it is needed: Apache Spark is designed for Linux/Unix systems. On Windows, it fails to perform file operations (like saving data or creating temporary buffers for BigQuery) because it cannot check file permissions natively. winutils.exe acts as a bridge, simulating the necessary Linux file system permissions so Spark can run without crashing."
   ],
   "id": "70db02e740237b22"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import sys\n",
    "\n",
    "def setup_windows_hadoop():\n",
    "    # 1. Define Paths\n",
    "    # We use C:/hadoop because it avoids space-in-path issues (like \"Program Files\")\n",
    "    base_dir = \"C:\\\\hadoop\"\n",
    "    bin_dir = os.path.join(base_dir, \"bin\")\n",
    "\n",
    "    # 2. Create Directories\n",
    "    print(f\"ðŸ“‚ Creating directory structure: {bin_dir}...\")\n",
    "    try:\n",
    "        os.makedirs(bin_dir, exist_ok=True)\n",
    "        print(\"   Directory created (or already exists).\")\n",
    "    except PermissionError:\n",
    "        print(\"âŒ ERROR: Permission denied. Try running PyCharm/Jupyter as Administrator.\")\n",
    "        return False\n",
    "\n",
    "    # 3. Define Files to Download (Standard Hadoop 3.2.2 binaries for Spark 3.x)\n",
    "    # Source: cdarlint/winutils (The standard community source)\n",
    "    base_url = \"https://github.com/cdarlint/winutils/raw/master/hadoop-3.2.2/bin/\"\n",
    "    files = [\"winutils.exe\", \"hadoop.dll\"]\n",
    "\n",
    "    # 4. Download Files\n",
    "    print(\"\\nâ¬‡ï¸ Downloading required binaries...\")\n",
    "    for filename in files:\n",
    "        target_path = os.path.join(bin_dir, filename)\n",
    "        url = base_url + filename\n",
    "\n",
    "        if os.path.exists(target_path):\n",
    "            print(f\"   âœ… {filename} already exists. Skipping.\")\n",
    "        else:\n",
    "            print(f\"   â³ Downloading {filename}...\")\n",
    "            try:\n",
    "                urllib.request.urlretrieve(url, target_path)\n",
    "                print(f\"   âœ… Downloaded {filename}\")\n",
    "            except Exception as e:\n",
    "                print(f\"   âŒ Failed to download {filename}: {e}\")\n",
    "                return False\n",
    "\n",
    "    # 5. Set Environment Variables for THIS session\n",
    "    print(\"\\nðŸ”§ Configuring Environment Variables...\")\n",
    "    os.environ['HADOOP_HOME'] = base_dir\n",
    "    # Add bin to PATH if not there\n",
    "    if bin_dir not in os.environ['PATH']:\n",
    "        os.environ['PATH'] = bin_dir + \";\" + os.environ['PATH']\n",
    "\n",
    "    print(\"âœ… SUCCESS: Hadoop environment is set up.\")\n",
    "    print(f\"   HADOOP_HOME = {os.environ['HADOOP_HOME']}\")\n",
    "    return True\n",
    "\n",
    "# Execute\n",
    "if sys.platform.startswith('win'):\n",
    "    setup_windows_hadoop()\n",
    "else:\n",
    "    print(\"Skipping Windows setup (Not running on Windows).\")"
   ],
   "id": "f747a32539840c21",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Pipeline continues here",
   "id": "f10b813e8e122004"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.functions import (\n",
    "    col, count, mean, stddev, desc, upper, lower,\n",
    "    current_timestamp, to_date, when, round, lit, substring, abs as spark_abs\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "\n",
    "# ==========================================\n",
    "# 1. CONTROL PANEL\n",
    "# ==========================================\n",
    "IS_LOCAL = True\n",
    "\n",
    "# TOGGLE THIS: True = 1.5GB file, False = 100 row file\n",
    "USE_FULL_DATASET = True\n",
    "\n",
    "# --- GOOGLE CLOUD CONFIG ---\n",
    "GCP_PROJECT_ID = \"your-project-id\"\n",
    "BQ_DATASET_NAME = \"vehicles_data\"\n",
    "BQ_TABLE_NAME = \"market_intelligence_system\"\n",
    "GCS_TEMP_BUCKET = \"your-staging-bucket\"\n",
    "\n",
    "# ==========================================\n",
    "# 2. NETWORK & PATH CONFIGURATION\n",
    "# ==========================================\n",
    "# Force Spark to bind to localhost (Fixes ConnectionRefusedError)\n",
    "os.environ[\"SPARK_LOCAL_IP\"] = \"127.0.0.1\"\n",
    "\n",
    "# Cleanup Lock Files from previous crashes\n",
    "print(\"ðŸ§¹ Cleaning up lock files...\")\n",
    "for path in [\"metastore_db\", \"derby.log\", \"spark-warehouse\"]:\n",
    "    if os.path.exists(path):\n",
    "        try:\n",
    "            if os.path.isdir(path): shutil.rmtree(path)\n",
    "            else: os.remove(path)\n",
    "        except: pass\n",
    "\n",
    "if IS_LOCAL:\n",
    "    print(\"ðŸ”§ Configuring for WINDOWS LOCALHOST...\")\n",
    "\n",
    "    # Force Paths\n",
    "    os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "    os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "    os.environ['HADOOP_HOME'] = \"C:/hadoop\"\n",
    "    os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = \"C:/Users/vanac/keys/my-service-account.json\"\n",
    "\n",
    "    # Define Input Paths\n",
    "    PATH_SMALL = \"C:/Users/vanac/Downloads/first100_vehicles.csv\"\n",
    "    PATH_FULL  = \"C:/Users/vanac/Downloads/vehicles.csv\"\n",
    "else:\n",
    "    print(\"â˜ï¸ Configuring for CLOUD/VM Environment...\")\n",
    "    PATH_SMALL = \"gs://your-bucket/input/first100_vehicles.csv\"\n",
    "    PATH_FULL  = \"gs://your-bucket/input/vehicles.csv\"\n",
    "\n",
    "# SELECT THE FILE BASED ON YOUR TOGGLE\n",
    "INPUT_FILE = PATH_FULL if USE_FULL_DATASET else PATH_SMALL\n",
    "\n",
    "# ==========================================\n",
    "# 3. INITIALIZE SPARK\n",
    "# ==========================================\n",
    "conf = SparkConf()\n",
    "conf.setAppName(\"Advanced_Market_Pipeline\")\n",
    "conf.set(\"spark.driver.memory\", \"2g\")\n",
    "conf.set(\"spark.jars.packages\", \"com.google.cloud.spark:spark-bigquery-with-dependencies_2.12:0.34.0\")\n",
    "\n",
    "if IS_LOCAL:\n",
    "    conf.setMaster(\"local[*]\")\n",
    "\n",
    "    # NETWORK FIXES (Crucial for Windows)\n",
    "    conf.set(\"spark.driver.bindAddress\", \"127.0.0.1\")\n",
    "    conf.set(\"spark.driver.host\", \"127.0.0.1\")\n",
    "\n",
    "    # TEMP WAREHOUSE FIX\n",
    "    os.makedirs(\"C:/temp/spark_warehouse\", exist_ok=True)\n",
    "    conf.set(\"spark.sql.warehouse.dir\", \"file:///C:/temp/spark_warehouse\")\n",
    "\n",
    "print(\"ðŸš€ Starting Spark Engine...\")\n",
    "\n",
    "try:\n",
    "    spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "    spark.sparkContext.setLogLevel(\"WARN\")\n",
    "    print(f\"âœ… SUCCESS: Spark is running.\")\n",
    "    print(f\"ðŸ“‚ Target File: {INPUT_FILE} (Full Dataset: {USE_FULL_DATASET})\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"\\nâŒ FAILED TO START\")\n",
    "    print(e)"
   ],
   "id": "e2d4157afecbf1de",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"--- Step 1: Ingestion ---\")\n",
    "# Load raw data\n",
    "df_raw = spark.read.csv(INPUT_FILE, header=True, inferSchema=True)\n",
    "print(f\"Raw Count: {df_raw.count()}\")"
   ],
   "id": "b22249c4a5194de2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"--- Step 2: Preprocessing & Standardization ---\")\n",
    "\n",
    "# 1. Standardize Strings (Remove whitespace, uppercase)\n",
    "df_prep = df_raw \\\n",
    "    .withColumn(\"manufacturer\", upper(col(\"manufacturer\"))) \\\n",
    "    .withColumn(\"model\", upper(col(\"model\"))) \\\n",
    "    .withColumn(\"title_status\", lower(col(\"title_status\"))) \\\n",
    "    .withColumn(\"vin\", upper(col(\"VIN\"))) # Standardize VIN column name\n",
    "\n",
    "# 2. Handle Nulls (Based on Assumptions)\n",
    "df_clean = df_prep \\\n",
    "    .dropna(subset=[\"manufacturer\", \"model\", \"year\", \"price\"]) \\\n",
    "    .filter(col(\"price\") > 500) \\\n",
    "    .filter(col(\"price\") < 200000) \\\n",
    "    .fillna({\"odometer\": -1, \"title_status\": \"clean\"}) # Assumption: Missing title = clean\n",
    "\n",
    "# 3. Type Casting\n",
    "df_clean = df_clean \\\n",
    "    .withColumn(\"year\", col(\"year\").cast(IntegerType())) \\\n",
    "    .withColumn(\"odometer\", col(\"odometer\").cast(DoubleType()))\n",
    "\n",
    "print(f\"Cleaned Count: {df_clean.count()}\")\n",
    "df_clean.select(\"manufacturer\", \"model\", \"year\", \"price\", \"odometer\", \"title_status\").show(5)"
   ],
   "id": "289780eb90ad0fdd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"--- Step 2: Statistical Scoring ---\")\n",
    "\n",
    "# 1. Define the Window\n",
    "# \"Group by Manufacturer, Model, and Year\"\n",
    "windowSpec = Window.partitionBy(\"manufacturer\", \"model\", \"year\")\n",
    "\n",
    "# 2. Calculate Group Statistics (Average & StdDev)\n",
    "df_stats = df_clean \\\n",
    "    .withColumn(\"market_avg_price\", round(mean(\"price\").over(windowSpec), 2)) \\\n",
    "    .withColumn(\"market_std_dev\", round(stddev(\"price\").over(windowSpec), 2)) \\\n",
    "    .withColumn(\"group_count\", count(\"id\").over(windowSpec))\n",
    "\n",
    "# 3. Calculate Z-Score\n",
    "# Formula: (Price - Average) / Standard Deviation\n",
    "# Logic: How far is this specific car from the 'normal' price?\n",
    "df_scored = df_stats.withColumn(\"price_z_score\",\n",
    "    (col(\"price\") - col(\"market_avg_price\")) / when(col(\"market_std_dev\") == 0, 1).otherwise(col(\"market_std_dev\"))\n",
    ")\n",
    "\n",
    "# 4. Assign Ratings\n",
    "df_scored_labeled = df_scored.withColumn(\"deal_rating\",\n",
    "    when(col(\"group_count\") < 5, \"INSUFFICIENT_DATA\")  # Skip rare cars\n",
    "    .when(col(\"price_z_score\") < -1.5, \"GREAT DEAL\")   # Cheap\n",
    "    .when(col(\"price_z_score\").between(-1.5, -0.5), \"GOOD PRICE\")\n",
    "    .when(col(\"price_z_score\").between(-0.5, 0.5), \"FAIR MARKET\")\n",
    "    .when(col(\"price_z_score\") > 1.5, \"OVERPRICED\")    # Expensive\n",
    "    .otherwise(\"SLIGHTLY HIGH\")\n",
    ")\n",
    "\n",
    "print(\"Scoring Complete. Preview:\")\n",
    "df_scored_labeled.select(\"manufacturer\", \"model\", \"price\", \"market_avg_price\", \"deal_rating\").show(10)"
   ],
   "id": "ac87d26ebf962b89",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"--- Step 3: VIN Forensics & Fraud Detection ---\")\n",
    "\n",
    "# 1. VIN Year Logic (Simplified for 2010-2029 codes)\n",
    "# Position 10 in VIN indicates year.\n",
    "# A=2010, B=2011 ... H=2017, J=2018, K=2019, L=2020, M=2021, N=2022, P=2023\n",
    "df_fraud_check = df_clean.withColumn(\"vin_year_code\", substring(col(\"vin\"), 10, 1))\n",
    "\n",
    "# Map code to year (SQL-style Case Statement)\n",
    "df_fraud_check = df_fraud_check.withColumn(\"decoded_vin_year\",\n",
    "    when(col(\"vin_year_code\") == \"A\", 2010)\n",
    "    .when(col(\"vin_year_code\") == \"B\", 2011)\n",
    "    .when(col(\"vin_year_code\") == \"C\", 2012)\n",
    "    .when(col(\"vin_year_code\") == \"D\", 2013)\n",
    "    .when(col(\"vin_year_code\") == \"E\", 2014)\n",
    "    .when(col(\"vin_year_code\") == \"F\", 2015)\n",
    "    .when(col(\"vin_year_code\") == \"G\", 2016)\n",
    "    .when(col(\"vin_year_code\") == \"H\", 2017)\n",
    "    .when(col(\"vin_year_code\") == \"J\", 2018)\n",
    "    .when(col(\"vin_year_code\") == \"K\", 2019)\n",
    "    .when(col(\"vin_year_code\") == \"L\", 2020)\n",
    "    .when(col(\"vin_year_code\") == \"M\", 2021)\n",
    "    .otherwise(0) # 0 means older than 2010 or not mapped\n",
    ")\n",
    "\n",
    "# 2. Fraud Logic\n",
    "# Flag 1: Year Mismatch (VIN says 2012, Listing says 2018)\n",
    "# Flag 2: Suspicious Low Mileage (Age > 5 years but odometer < 2000 miles)\n",
    "current_yr = 2025\n",
    "df_fraud_check = df_fraud_check \\\n",
    "    .withColumn(\"is_year_mismatch\",\n",
    "        when((col(\"decoded_vin_year\") > 0) & (spark_abs(col(\"decoded_vin_year\") - col(\"year\")) > 1), True)\n",
    "        .otherwise(False)\n",
    "    ) \\\n",
    "    .withColumn(\"is_suspicious_mileage\",\n",
    "        when((col(\"year\") < (current_yr - 5)) & (col(\"odometer\") > 0) & (col(\"odometer\") < 2000), True)\n",
    "        .otherwise(False)\n",
    "    )\n",
    "\n",
    "# Trust Score (0 to 100)\n",
    "# Start at 100. Subtract 50 for Year Mismatch. Subtract 30 for Suspicious Mileage.\n",
    "df_fraud_check = df_fraud_check.withColumn(\"trust_score\",\n",
    "    lit(100) -\n",
    "    when(col(\"is_year_mismatch\"), 50).otherwise(0) -\n",
    "    when(col(\"is_suspicious_mileage\"), 30).otherwise(0)\n",
    ")\n",
    "\n",
    "print(\"Fraud Detection Preview (Low Trust Items):\")\n",
    "df_fraud_check.filter(col(\"trust_score\") < 100).select(\"vin\", \"year\", \"decoded_vin_year\", \"odometer\", \"trust_score\").show(5)"
   ],
   "id": "78474e3f592fb52e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"--- Step 4: Geospatial Arbitrage ---\")\n",
    "\n",
    "# 1. Calculate National Average for every Make/Model/Year\n",
    "national_window = Window.partitionBy(\"manufacturer\", \"model\", \"year\")\n",
    "df_geo = df_fraud_check.withColumn(\"national_avg_price\", round(mean(\"price\").over(national_window), 2))\n",
    "\n",
    "# 2. Calculate Regional Arbitrage\n",
    "# Arbitrage Value = National Average - Local Price\n",
    "# Positive Value = Cheaper to buy here (Good for flipping)\n",
    "# Negative Value = More expensive here (Bad for buying)\n",
    "df_geo = df_geo.withColumn(\"arbitrage_opportunity\", col(\"national_avg_price\") - col(\"price\"))\n",
    "\n",
    "# 3. Flag High-Value Arbitrage Regions\n",
    "# If you can save > $2000 by buying here vs national average, it's a \"Sourcing Hub\"\n",
    "df_geo = df_geo.withColumn(\"is_sourcing_hub\", when(col(\"arbitrage_opportunity\") > 2000, True).otherwise(False))\n",
    "\n",
    "print(\"Arbitrage Preview (Top Opportunities):\")\n",
    "df_geo.filter(\"is_sourcing_hub\").select(\"region\", \"manufacturer\", \"model\", \"price\", \"national_avg_price\", \"arbitrage_opportunity\").show(5)"
   ],
   "id": "39d261e9c020f10f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"--- Step 5: Adjusted Deal Scoring ---\")\n",
    "\n",
    "windowSpec = Window.partitionBy(\"manufacturer\", \"model\", \"year\")\n",
    "\n",
    "df_scored = df_geo \\\n",
    "    .withColumn(\"market_std_dev\", round(stddev(\"price\").over(windowSpec), 2)) \\\n",
    "    .withColumn(\"group_count\", count(\"id\").over(windowSpec))\n",
    "\n",
    "# Z-Score Calculation\n",
    "df_scored = df_scored.withColumn(\"price_z_score\",\n",
    "    (col(\"price\") - col(\"national_avg_price\")) / when(col(\"market_std_dev\") == 0, 1).otherwise(col(\"market_std_dev\"))\n",
    ")\n",
    "\n",
    "# Rating Logic (Integrated with Fraud Check)\n",
    "# If Trust Score is Low, it cannot be a \"Great Deal\" (It's a trap!)\n",
    "df_final = df_scored.withColumn(\"deal_rating\",\n",
    "    when(col(\"trust_score\") < 70, \"RISKY_BUY\") # New Category\n",
    "    .when(col(\"group_count\") < 5, \"INSUFFICIENT_DATA\")\n",
    "    .when(col(\"price_z_score\") < -1.5, \"GREAT_DEAL\")\n",
    "    .when(col(\"price_z_score\").between(-1.5, -0.5), \"GOOD_PRICE\")\n",
    "    .when(col(\"price_z_score\").between(-0.5, 0.5), \"FAIR_MARKET\")\n",
    "    .otherwise(\"OVERPRICED\")\n",
    ")\n",
    "\n",
    "print(\"Final Scored Data:\")\n",
    "df_final.select(\"manufacturer\", \"model\", \"price\", \"trust_score\", \"arbitrage_opportunity\", \"deal_rating\").show(5)"
   ],
   "id": "3aa72bb641bafcae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"--- Step 6: Writing to BigQuery ---\")\n",
    "\n",
    "full_table_id = f\"{GCP_PROJECT_ID}.{BQ_DATASET_NAME}.{BQ_TABLE_NAME}\"\n",
    "\n",
    "try:\n",
    "    # Select only valuable columns\n",
    "    output_df = df_final.select(\n",
    "        \"id\", \"manufacturer\", \"model\", \"year\", \"price\", \"odometer\", \"region\", \"lat\", \"long\",\n",
    "        \"vin\", \"trust_score\", \"is_year_mismatch\", # Fraud features\n",
    "        \"national_avg_price\", \"arbitrage_opportunity\", \"is_sourcing_hub\", # Geo features\n",
    "        \"deal_rating\", # Final score\n",
    "        current_timestamp().alias(\"processed_at\")\n",
    "    )\n",
    "\n",
    "    print(f\"ðŸš€ Writing to: {full_table_id}\")\n",
    "    output_df.write \\\n",
    "        .format(\"bigquery\") \\\n",
    "        .option(\"table\", full_table_id) \\\n",
    "        .option(\"temporaryGcsBucket\", GCS_TEMP_BUCKET) \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .save()\n",
    "\n",
    "    print(\"âœ… SUCCESS! Market Intelligence Data is in BigQuery.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"âŒ ERROR writing to BigQuery.\")\n",
    "    print(e)"
   ],
   "id": "448e925ab2ebcb74",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "print(\"ðŸ“‚ Your notebook is saved here:\")\n",
    "print(os.getcwd())"
   ],
   "id": "c65d6fb5b6ea3d63",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
