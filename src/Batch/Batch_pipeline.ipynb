{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e695889-19d4-4016-a2c2-35a5f747a632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Windows setup (Not running on Windows).\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import sys\n",
    "\n",
    "def setup_windows_hadoop():\n",
    "    # 1. Define Paths\n",
    "    # We use C:/hadoop because it avoids space-in-path issues (like \"Program Files\")\n",
    "    base_dir = \"C:\\\\hadoop\"\n",
    "    bin_dir = os.path.join(base_dir, \"bin\")\n",
    "\n",
    "    # 2. Create Directories\n",
    "    print(f\"Creating directory structure: {bin_dir}...\")\n",
    "    try:\n",
    "        os.makedirs(bin_dir, exist_ok=True)\n",
    "        print(\"   Directory created (or already exists).\")\n",
    "    except PermissionError:\n",
    "        print(\" ERROR: Permission denied. Try running PyCharm/Jupyter as Administrator.\")\n",
    "        return False\n",
    "\n",
    "    # 3. Define Files to Download (Standard Hadoop 3.2.2 binaries for Spark 3.x)\n",
    "    # Source: cdarlint/winutils (The standard community source)\n",
    "    base_url = \"https://github.com/cdarlint/winutils/raw/master/hadoop-3.2.2/bin/\"\n",
    "    files = [\"winutils.exe\", \"hadoop.dll\"]\n",
    "\n",
    "    # 4. Download Files\n",
    "    print(\"\\n Downloading required binaries...\")\n",
    "    for filename in files:\n",
    "        target_path = os.path.join(bin_dir, filename)\n",
    "        url = base_url + filename\n",
    "\n",
    "        if os.path.exists(target_path):\n",
    "            print(f\"    {filename} already exists. Skipping.\")\n",
    "        else:\n",
    "            print(f\"    Downloading {filename}...\")\n",
    "            try:\n",
    "                urllib.request.urlretrieve(url, target_path)\n",
    "                print(f\"    Downloaded {filename}\")\n",
    "            except Exception as e:\n",
    "                print(f\"    Failed to download {filename}: {e}\")\n",
    "                return False\n",
    "\n",
    "    # 5. Set Environment Variables for THIS session\n",
    "    print(\"\\nðŸ”§ Configuring Environment Variables...\")\n",
    "    os.environ['HADOOP_HOME'] = base_dir\n",
    "    # Add bin to PATH if not there\n",
    "    if bin_dir not in os.environ['PATH']:\n",
    "        os.environ['PATH'] = bin_dir + \";\" + os.environ['PATH']\n",
    "\n",
    "    print(\" SUCCESS: Hadoop environment is set up.\")\n",
    "    print(f\"   HADOOP_HOME = {os.environ['HADOOP_HOME']}\")\n",
    "    return True\n",
    "\n",
    "# Execute\n",
    "if sys.platform.startswith('win'):\n",
    "    setup_windows_hadoop()\n",
    "else:\n",
    "    print(\"Skipping Windows setup (Not running on Windows).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9653e3c6-f22c-4059-920b-676f18b35b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â˜ï¸ Configuring for CLOUD/VM Environment...\n",
      "ðŸš€ Starting Spark Engine...\n",
      ":: loading settings :: url = jar:file:/home/vanac/.local/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/vanac/.ivy2/cache\n",
      "The jars for the packages stored in: /home/vanac/.ivy2/jars\n",
      "com.google.cloud.spark#spark-bigquery-with-dependencies_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-7c1dd641-323a-41a4-9eaa-ef8ede938807;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.google.cloud.spark#spark-bigquery-with-dependencies_2.12;0.36.1 in central\n",
      ":: resolution report :: resolve 424ms :: artifacts dl 15ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.cloud.spark#spark-bigquery-with-dependencies_2.12;0.36.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   1   |   0   |   0   |   0   ||   1   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-7c1dd641-323a-41a4-9eaa-ef8ede938807\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 1 already retrieved (0kB/20ms)\n",
      "25/11/24 10:37:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " SUCCESS: Spark 3.4.1 is running.\n",
      " Target File: gs://vehicles_csv/vehicles.csv\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "\n",
    "# ==========================================\n",
    "# 1. PRODUCTION CONFIG\n",
    "# ==========================================\n",
    "IS_LOCAL = False          \n",
    "USE_FULL_DATASET = True   \n",
    "\n",
    "# --- GOOGLE CLOUD CONFIG ---\n",
    "GCP_PROJECT_ID = \"de2025-472211\"\n",
    "BQ_DATASET_NAME = \"vehicles_data\"\n",
    "BQ_TABLE_NAME = \"market_intelligence_system\"\n",
    "GCS_TEMP_BUCKET = \"vehicles_csv\"  # Bucket Name\n",
    "\n",
    "# ==========================================\n",
    "# 2. ENVIRONMENT CONFIGURATION\n",
    "# ==========================================\n",
    "os.environ[\"SPARK_LOCAL_IP\"] = \"127.0.0.1\"\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
    "\n",
    "if IS_LOCAL:\n",
    "    pass\n",
    "else:\n",
    "    print(\"â˜ï¸ Configuring for CLOUD/VM Environment...\")\n",
    "    os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "    os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "    \n",
    "    # --- FIX: REMOVED '/input' FOLDER FROM PATHS ---\n",
    "    # Based on your gsutil ls output: gs://vehicles_csv/vehicles.csv\n",
    "    PATH_SMALL = f\"gs://{GCS_TEMP_BUCKET}/first100_vehicles.csv\" \n",
    "    PATH_FULL  = f\"gs://{GCS_TEMP_BUCKET}/vehicles.csv\"\n",
    "\n",
    "INPUT_FILE = PATH_FULL if USE_FULL_DATASET else PATH_SMALL\n",
    "\n",
    "# ==========================================\n",
    "# 3. INITIALIZE SPARK\n",
    "# ==========================================\n",
    "conf = SparkConf()\n",
    "conf.setAppName(\"Advanced_Market_Pipeline\")\n",
    "conf.set(\"spark.driver.memory\", \"4g\")\n",
    "\n",
    "# --- DEPENDENCIES ---\n",
    "# 1. BigQuery Connector (Maven)\n",
    "conf.set(\"spark.jars.packages\", \"com.google.cloud.spark:spark-bigquery-with-dependencies_2.12:0.36.1\")\n",
    "\n",
    "# 2. GCS Connector (Local Shaded JAR)\n",
    "# Using the file you downloaded manually to /home/vanac\n",
    "conf.set(\"spark.jars\", \"/home/vanac/gcs-connector-hadoop3-2.2.22-shaded.jar\")\n",
    "\n",
    "# Configure Hadoop\n",
    "conf.set(\"spark.hadoop.fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "conf.set(\"spark.hadoop.fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n",
    "conf.set(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\")\n",
    "\n",
    "print(f\"ðŸš€ Starting Spark Engine...\")\n",
    "\n",
    "try:\n",
    "    spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "    spark.sparkContext.setLogLevel(\"WARN\")\n",
    "    print(f\" SUCCESS: Spark {spark.version} is running.\")\n",
    "    print(f\" Target File: {INPUT_FILE}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"\\n FAILED TO START\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "233710dd-6eae-409b-88ba-5ee8e6bd340c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 1: Ingestion ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:===============================================>          (9 + 2) / 11]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Count: 441802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "print(\"--- Step 1: Ingestion ---\")\n",
    "# Load raw data\n",
    "df_raw = spark.read.csv(INPUT_FILE, header=True, inferSchema=True)\n",
    "print(f\"Raw Count: {df_raw.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91076ee3-b4c7-4d50-8b6c-5207d26af37d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 2: Preprocessing & Standardization ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Count: 364062\n",
      "+------------+--------------------+----+-----+--------+------------+\n",
      "|manufacturer|               model|year|price|odometer|title_status|\n",
      "+------------+--------------------+----+-----+--------+------------+\n",
      "|         GMC|SIERRA 1500 CREW ...|2014|33590| 57923.0|       clean|\n",
      "|   CHEVROLET|      SILVERADO 1500|2010|22590| 71229.0|       clean|\n",
      "|   CHEVROLET| SILVERADO 1500 CREW|2020|39590| 19160.0|       clean|\n",
      "|      TOYOTA|TUNDRA DOUBLE CAB SR|2017|30990| 41124.0|       clean|\n",
      "|        FORD|           F-150 XLT|2013|15000|128000.0|       clean|\n",
      "+------------+--------------------+----+-----+--------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, upper, lower\n",
    "from pyspark.sql.types import IntegerType, DoubleType\n",
    "\n",
    "print(\"--- Step 2: Preprocessing & Standardization ---\")\n",
    "\n",
    "# 1. Standardize Strings (Remove whitespace, uppercase)\n",
    "df_prep = df_raw \\\n",
    "    .withColumn(\"manufacturer\", upper(col(\"manufacturer\"))) \\\n",
    "    .withColumn(\"model\", upper(col(\"model\"))) \\\n",
    "    .withColumn(\"title_status\", lower(col(\"title_status\"))) \\\n",
    "    .withColumn(\"vin\", upper(col(\"VIN\"))) # Standardize VIN column name\n",
    "\n",
    "# 2. Handle Nulls (Based on Assumptions)\n",
    "df_clean = df_prep \\\n",
    "    .dropna(subset=[\"manufacturer\", \"model\", \"year\", \"price\"]) \\\n",
    "    .filter(col(\"price\") > 500) \\\n",
    "    .filter(col(\"price\") < 200000) \\\n",
    "    .fillna({\"odometer\": -1, \"title_status\": \"clean\"}) # Assumption: Missing title = clean\n",
    "\n",
    "# 3. Type Casting\n",
    "df_clean = df_clean \\\n",
    "    .withColumn(\"year\", col(\"year\").cast(IntegerType())) \\\n",
    "    .withColumn(\"odometer\", col(\"odometer\").cast(DoubleType()))\n",
    "\n",
    "print(f\"Cleaned Count: {df_clean.count()}\")\n",
    "df_clean.select(\"manufacturer\", \"model\", \"year\", \"price\", \"odometer\", \"title_status\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd541bbf-9eb1-4a37-a4d2-fa3eecdb9075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 2: Statistical Scoring ---\n",
      "Scoring Complete. Preview:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+-----+----------------+-----------+\n",
      "|manufacturer|model|price|market_avg_price|deal_rating|\n",
      "+------------+-----+-----+----------------+-----------+\n",
      "|        2015| 2016| 2013|          2013.0|FAIR MARKET|\n",
      "|        2015| 2016| 2013|          2013.0|FAIR MARKET|\n",
      "|        2015| 2016| 2013|          2013.0|FAIR MARKET|\n",
      "|        2015| 2016| 2013|          2013.0|FAIR MARKET|\n",
      "|        2015| 2016| 2013|          2013.0|FAIR MARKET|\n",
      "|        2015| 2016| 2013|          2013.0|FAIR MARKET|\n",
      "|        2015| 2016| 2013|          2013.0|FAIR MARKET|\n",
      "|        2015| 2016| 2013|          2013.0|FAIR MARKET|\n",
      "|        2015| 2016| 2013|          2013.0|FAIR MARKET|\n",
      "|        2015| 2016| 2013|          2013.0|FAIR MARKET|\n",
      "+------------+-----+-----+----------------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "print(\"--- Step 2: Statistical Scoring ---\")\n",
    "\n",
    "# 1. Define the Window\n",
    "# \"Group by Manufacturer, Model, and Year\"\n",
    "windowSpec = Window.partitionBy(\"manufacturer\", \"model\", \"year\")\n",
    "\n",
    "# 2. Calculate Group Statistics (Average & StdDev)\n",
    "df_stats = df_clean \\\n",
    "    .withColumn(\"market_avg_price\", round(mean(\"price\").over(windowSpec), 2)) \\\n",
    "    .withColumn(\"market_std_dev\", round(stddev(\"price\").over(windowSpec), 2)) \\\n",
    "    .withColumn(\"group_count\", count(\"id\").over(windowSpec))\n",
    "\n",
    "# 3. Calculate Z-Score\n",
    "# Formula: (Price - Average) / Standard Deviation\n",
    "# Logic: How far is this specific car from the 'normal' price?\n",
    "df_scored = df_stats.withColumn(\"price_z_score\",\n",
    "    (col(\"price\") - col(\"market_avg_price\")) / when(col(\"market_std_dev\") == 0, 1).otherwise(col(\"market_std_dev\"))\n",
    ")\n",
    "\n",
    "# 4. Assign Ratings\n",
    "df_scored_labeled = df_scored.withColumn(\"deal_rating\",\n",
    "    when(col(\"group_count\") < 5, \"INSUFFICIENT_DATA\")  # Skip rare cars\n",
    "    .when(col(\"price_z_score\") < -1.5, \"GREAT DEAL\")   # Cheap\n",
    "    .when(col(\"price_z_score\").between(-1.5, -0.5), \"GOOD PRICE\")\n",
    "    .when(col(\"price_z_score\").between(-0.5, 0.5), \"FAIR MARKET\")\n",
    "    .when(col(\"price_z_score\") > 1.5, \"OVERPRICED\")    # Expensive\n",
    "    .otherwise(\"SLIGHTLY HIGH\")\n",
    ")\n",
    "\n",
    "print(\"Scoring Complete. Preview:\")\n",
    "df_scored_labeled.select(\"manufacturer\", \"model\", \"price\", \"market_avg_price\", \"deal_rating\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6182b760-08a1-4e32-a005-d7b8887f7b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 3: VIN Forensics & Fraud Detection ---\n",
      "Fraud Detection Preview (Low Trust Items):\n",
      "+-----------------+----+----------------+--------+-----------+\n",
      "|              vin|year|decoded_vin_year|odometer|trust_score|\n",
      "+-----------------+----+----------------+--------+-----------+\n",
      "|1FTER4EH3KLA31326|2019|            2019|  1834.0|         70|\n",
      "|1C4GJXAGXKW566358|2019|            2019|  1423.0|         70|\n",
      "|             null|1976|               0|   100.0|         70|\n",
      "|             null|1979|               0|  1234.0|         70|\n",
      "|1C4AJWAG1HL698869|2017|            2017|   891.0|         70|\n",
      "+-----------------+----+----------------+--------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, substring, when, abs as spark_abs, lit\n",
    "\n",
    "print(\"--- Step 3: VIN Forensics & Fraud Detection ---\")\n",
    "\n",
    "# 1. VIN Year Logic (Simplified for 2010-2029 codes)\n",
    "# Position 10 in VIN indicates year. \n",
    "# A=2010, B=2011 ... H=2017, J=2018, K=2019, L=2020, M=2021, N=2022, P=2023\n",
    "df_fraud_check = df_clean.withColumn(\"vin_year_code\", substring(col(\"vin\"), 10, 1))\n",
    "\n",
    "# Map code to year (SQL-style Case Statement)\n",
    "df_fraud_check = df_fraud_check.withColumn(\"decoded_vin_year\",\n",
    "    when(col(\"vin_year_code\") == \"A\", 2010)\n",
    "    .when(col(\"vin_year_code\") == \"B\", 2011)\n",
    "    .when(col(\"vin_year_code\") == \"C\", 2012)\n",
    "    .when(col(\"vin_year_code\") == \"D\", 2013)\n",
    "    .when(col(\"vin_year_code\") == \"E\", 2014)\n",
    "    .when(col(\"vin_year_code\") == \"F\", 2015)\n",
    "    .when(col(\"vin_year_code\") == \"G\", 2016)\n",
    "    .when(col(\"vin_year_code\") == \"H\", 2017)\n",
    "    .when(col(\"vin_year_code\") == \"J\", 2018)\n",
    "    .when(col(\"vin_year_code\") == \"K\", 2019)\n",
    "    .when(col(\"vin_year_code\") == \"L\", 2020)\n",
    "    .when(col(\"vin_year_code\") == \"M\", 2021)\n",
    "    .when(col(\"vin_year_code\") == \"N\", 2022)\n",
    "    .when(col(\"vin_year_code\") == \"P\", 2023)\n",
    "    .otherwise(0) # 0 means older than 2010 or not mapped\n",
    ")\n",
    "\n",
    "# 2. Fraud Logic\n",
    "# Flag 1: Year Mismatch (VIN says 2012, Listing says 2018)\n",
    "# Flag 2: Suspicious Low Mileage (Age > 5 years but odometer < 2000 miles)\n",
    "current_yr = 2025\n",
    "df_fraud_check = df_fraud_check \\\n",
    "    .withColumn(\"is_year_mismatch\", \n",
    "        when((col(\"decoded_vin_year\") > 0) & (spark_abs(col(\"decoded_vin_year\") - col(\"year\")) > 1), True)\n",
    "        .otherwise(False)\n",
    "    ) \\\n",
    "    .withColumn(\"is_suspicious_mileage\", \n",
    "        when((col(\"year\") < (current_yr - 5)) & (col(\"odometer\") > 0) & (col(\"odometer\") < 2000), True)\n",
    "        .otherwise(False)\n",
    "    )\n",
    "\n",
    "# Trust Score (0 to 100)\n",
    "# Start at 100. Subtract 50 for Year Mismatch. Subtract 30 for Suspicious Mileage.\n",
    "df_fraud_check = df_fraud_check.withColumn(\"trust_score\", \n",
    "    lit(100) - \n",
    "    when(col(\"is_year_mismatch\"), 50).otherwise(0) - \n",
    "    when(col(\"is_suspicious_mileage\"), 30).otherwise(0)\n",
    ")\n",
    "\n",
    "print(\"Fraud Detection Preview (Low Trust Items):\")\n",
    "df_fraud_check.filter(col(\"trust_score\") < 100).select(\"vin\", \"year\", \"decoded_vin_year\", \"odometer\", \"trust_score\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "abbeaf49-e72a-4ab2-a66c-f2283d08d04c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 4: Geospatial Arbitrage ---\n",
      "Arbitrage Preview (Top Opportunities):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 15:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+-------------+-----+------------------+---------------------+\n",
      "|              region|manufacturer|        model|price|national_avg_price|arbitrage_opportunity|\n",
      "+--------------------+------------+-------------+-----+------------------+---------------------+\n",
      "|macon / warner ro...|       ACURA|3.2 CL TYPE S| 6000|            8389.4|   2389.3999999999996|\n",
      "|             chicago|       ACURA|3.2 CL TYPE S| 5950|            8389.4|   2439.3999999999996|\n",
      "|     fresno / madera|       ACURA|          ILX| 8000|          10949.53|   2949.5300000000007|\n",
      "|ft myers / SW flo...|       ACURA|          ILX| 7988|          10949.53|   2961.5300000000007|\n",
      "|       south florida|       ACURA|          ILX| 7900|          10949.53|   3049.5300000000007|\n",
      "+--------------------+------------+-------------+-----+------------------+---------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "print(\"--- Step 4: Geospatial Arbitrage ---\")\n",
    "\n",
    "# 1. Calculate National Average for every Make/Model/Year\n",
    "national_window = Window.partitionBy(\"manufacturer\", \"model\", \"year\")\n",
    "df_geo = df_fraud_check.withColumn(\"national_avg_price\", round(mean(\"price\").over(national_window), 2))\n",
    "\n",
    "# 2. Calculate Regional Arbitrage\n",
    "# Arbitrage Value = National Average - Local Price\n",
    "# Positive Value = Cheaper to buy here (Good for flipping)\n",
    "# Negative Value = More expensive here (Bad for buying)\n",
    "df_geo = df_geo.withColumn(\"arbitrage_opportunity\", col(\"national_avg_price\") - col(\"price\"))\n",
    "\n",
    "# 3. Flag High-Value Arbitrage Regions\n",
    "# If you can save > $2000 by buying here vs national average, it's a \"Sourcing Hub\"\n",
    "df_geo = df_geo.withColumn(\"is_sourcing_hub\", when(col(\"arbitrage_opportunity\") > 2000, True).otherwise(False))\n",
    "\n",
    "print(\"Arbitrage Preview (Top Opportunities):\")\n",
    "df_geo.filter(\"is_sourcing_hub\").select(\"region\", \"manufacturer\", \"model\", \"price\", \"national_avg_price\", \"arbitrage_opportunity\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98997dd8-bf41-4e4a-a3b9-73a09b3f7d11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 5: Adjusted Deal Scoring ---\n",
      "Final Scored Data:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:==================================================>     (10 + 1) / 11]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+-----+-----------+---------------------+-----------+\n",
      "|manufacturer|model|price|trust_score|arbitrage_opportunity|deal_rating|\n",
      "+------------+-----+-----+-----------+---------------------+-----------+\n",
      "|        2015| 2016| 2013|        100|                  0.0|FAIR_MARKET|\n",
      "|        2015| 2016| 2013|        100|                  0.0|FAIR_MARKET|\n",
      "|        2015| 2016| 2013|        100|                  0.0|FAIR_MARKET|\n",
      "|        2015| 2016| 2013|        100|                  0.0|FAIR_MARKET|\n",
      "|        2015| 2016| 2013|        100|                  0.0|FAIR_MARKET|\n",
      "+------------+-----+-----+-----------+---------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "print(\"--- Step 5: Adjusted Deal Scoring ---\")\n",
    "\n",
    "windowSpec = Window.partitionBy(\"manufacturer\", \"model\", \"year\")\n",
    "\n",
    "df_scored = df_geo \\\n",
    "    .withColumn(\"market_std_dev\", round(stddev(\"price\").over(windowSpec), 2)) \\\n",
    "    .withColumn(\"group_count\", count(\"id\").over(windowSpec))\n",
    "\n",
    "# Z-Score Calculation\n",
    "df_scored = df_scored.withColumn(\"price_z_score\",\n",
    "    (col(\"price\") - col(\"national_avg_price\")) / when(col(\"market_std_dev\") == 0, 1).otherwise(col(\"market_std_dev\"))\n",
    ")\n",
    "\n",
    "# Rating Logic (Integrated with Fraud Check)\n",
    "# If Trust Score is Low, it cannot be a \"Great Deal\" (It's a trap!)\n",
    "df_final = df_scored.withColumn(\"deal_rating\",\n",
    "    when(col(\"trust_score\") < 70, \"RISKY_BUY\") # New Category\n",
    "    .when(col(\"group_count\") < 5, \"INSUFFICIENT_DATA\")\n",
    "    .when(col(\"price_z_score\") < -1.5, \"GREAT_DEAL\")\n",
    "    .when(col(\"price_z_score\").between(-1.5, -0.5), \"GOOD_PRICE\")\n",
    "    .when(col(\"price_z_score\").between(-0.5, 0.5), \"FAIR_MARKET\")\n",
    "    .otherwise(\"OVERPRICED\")\n",
    ")\n",
    "\n",
    "print(\"Final Scored Data:\")\n",
    "df_final.select(\"manufacturer\", \"model\", \"price\", \"trust_score\", \"arbitrage_opportunity\", \"deal_rating\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f02c718-ae8b-4c82-b827-4fb19fcc912c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 6: Writing to BigQuery ---\n",
      "ðŸš€ Writing to: de2025-472211.vehicles_data.market_intelligence_system\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " SUCCESS! Market Intelligence Data is in BigQuery.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "print(\"--- Step 6: Writing to BigQuery ---\")\n",
    "\n",
    "full_table_id = f\"{GCP_PROJECT_ID}.{BQ_DATASET_NAME}.{BQ_TABLE_NAME}\"\n",
    "\n",
    "try:\n",
    "    # Select only valuable columns\n",
    "    output_df = df_final.select(\n",
    "        \"id\", \"manufacturer\", \"model\", \"year\", \"price\", \"odometer\", \"region\", \"lat\", \"long\",\n",
    "        \"vin\", \"trust_score\", \"is_year_mismatch\", # Fraud features\n",
    "        \"national_avg_price\", \"arbitrage_opportunity\", \"is_sourcing_hub\", # Geo features\n",
    "        \"deal_rating\", # Final score\n",
    "        current_timestamp().alias(\"processed_at\")\n",
    "    )\n",
    "    \n",
    "    print(f\"ðŸš€ Writing to: {full_table_id}\")\n",
    "    output_df.write \\\n",
    "        .format(\"bigquery\") \\\n",
    "        .option(\"table\", full_table_id) \\\n",
    "        .option(\"temporaryGcsBucket\", GCS_TEMP_BUCKET) \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .save()\n",
    "        \n",
    "    print(\" SUCCESS! Market Intelligence Data is in BigQuery.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\" ERROR writing to BigQuery.\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be451a8-f154-41f7-a4af-aa6de8db16ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
